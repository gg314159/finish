{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRI PET ECA融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\" \n",
    "import time\n",
    "# import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import pickle as p\n",
    "import hiddenlayer as hl\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda is available\")\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root,transform_mri= None,transform_pet = None):\n",
    "        super(MyDataset, self).__init__()\n",
    "        MRI_PET_match_all = pd.read_excel(root)\n",
    "        MRI = []\n",
    "        PET = []\n",
    "        group = []\n",
    "        for index,row in MRI_PET_match_all.iterrows():\n",
    "            mridata = cv2.imread(row['mripath']) \n",
    "            petdata = cv2.imread(row['petpath']) \n",
    "            MRI.append(mridata)\n",
    "            PET.append(petdata)\n",
    "            group.append(row['Group'])\n",
    "        self.MRI = MRI\n",
    "        self.PET = PET\n",
    "        self.group = group  \n",
    "        self.transform_mri = transform_mri\n",
    "        self.transform_pet = transform_pet\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        pppet = self.PET[index]\n",
    "        ppet = np.resize(pppet,(436,364,3))\n",
    "        mri = torch.from_numpy(self.MRI[index].transpose(2,0,1)).float()\n",
    "        pet = torch.from_numpy(ppet.transpose(2,0,1)).float()\n",
    "        mri = self.transform_mri(mri)\n",
    "        pet = self.transform_pet(pet)\n",
    "\n",
    "        group = self.group[index]\n",
    "        \n",
    "        return mri,pet,group\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.MRI)\n",
    "    \n",
    "    def get_classes_for_all_imgs(self):\n",
    "        return self.group\n",
    "    \n",
    "\n",
    "## transforms模块，进行数据预处理\n",
    "\n",
    "\n",
    "# not16\n",
    "# train_mean_mri = [4.1620684, 4.1620684, 4.1620684]\n",
    "# train_std_mri = [5.2131376, 5.2131376, 5.2131376]\n",
    "# train_mean_pet = [4.081158, 4.081158, 4.081158] \n",
    "# train_std_pet = [5.1888165, 5.1888165, 5.1888165]\n",
    "\n",
    "# test_mean_mri = [4.1623616, 4.1623616, 4.1623616]\n",
    "# test_std_mri = [5.2136188, 5.2136188, 5.2136188]\n",
    "# test_mean_pet = [4.106387, 4.106387, 4.106387]\n",
    "# test_std_pet = [5.18535, 5.18535, 5.18535]\n",
    "\n",
    "\n",
    "\n",
    "#only one\n",
    "\n",
    "# train_mean_mri = [4.240719, 4.240719, 4.240719]\n",
    "# train_std_mri = [5.254782, 5.254782, 5.254782]\n",
    "# train_mean_pet = [4.106597, 4.106597, 4.106597] \n",
    "# train_std_pet = [5.21822, 5.21822, 5.21822]\n",
    "\n",
    "# test_mean_mri = [4.364004, 4.364004, 4.364004]\n",
    "# test_std_mri = [5.4076633, 5.4076633, 5.4076633] \n",
    "# test_mean_pet = [4.2559123, 4.2559123, 4.2559123]\n",
    "# test_std_pet = [5.373126, 5.373126, 5.373126]\n",
    "\n",
    "# valid_mean_mri = [4.7232037, 4.7232037, 4.7232037]\n",
    "# valid_std_mri =  [5.8529644, 5.8529644, 5.8529644] \n",
    "# valid_mean_pet = [4.5747848, 4.5747848, 4.5747848]\n",
    "# valid_std_pet = [5.809622, 5.809622, 5.809622]\n",
    "\n",
    "#16-in-1\n",
    "train_mean_mri = [4.176061, 4.176061, 4.176061]\n",
    "train_std_mri = [5.231413, 5.231413, 5.231413]\n",
    "train_mean_pet = [4.017313, 4.017313, 4.017313] \n",
    "train_std_pet = [5.1714053, 5.1714053, 5.1714053]\n",
    "\n",
    "test_mean_mri = [4.3262687, 4.3262687, 4.3262687]\n",
    "test_std_mri = [5.419836, 5.419836, 5.419836] \n",
    "test_mean_pet = [4.1623745, 4.1623745, 4.1623745]\n",
    "test_std_pet = [5.3586817, 5.3586817, 5.3586817]\n",
    "\n",
    "valid_mean_mri = [4.7232037, 4.7232037, 4.7232037]\n",
    "valid_std_mri =  [5.8529644, 5.8529644, 5.8529644] \n",
    "valid_mean_pet = [4.5747848, 4.5747848, 4.5747848]\n",
    "valid_std_pet = [5.809622, 5.809622, 5.809622]\n",
    "\n",
    "\n",
    "\n",
    "train_transform_mri = transforms.Compose([\n",
    "    transforms.Normalize(train_mean_mri,train_std_mri),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    \n",
    "])\n",
    "\n",
    "train_transform_pet = transforms.Compose([\n",
    "    transforms.Normalize(train_mean_pet,train_std_pet),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "])\n",
    "\n",
    "test_transform_mri = transforms.Compose([\n",
    "    \n",
    "    transforms.Normalize(test_mean_mri,test_std_mri),\n",
    "])\n",
    "\n",
    "test_transform_pet = transforms.Compose([\n",
    "   \n",
    "    transforms.Normalize(test_mean_pet,test_std_pet)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data = MyDataset(\"/home/gc/gechang/final_final/filelist/two_class/ADCN/trainadcn.xlsx\", transform_mri =train_transform_mri,transform_pet =train_transform_pet )\n",
    "test_data = MyDataset(\"/home/gc/gechang/final_final/filelist/two_class/ADCN/testadcn.xls\", transform_mri =test_transform_mri,transform_pet =test_transform_pet)\n",
    "\n",
    "\n",
    "\n",
    "# 数据集中，每一类的数目。\n",
    "#class_sample_counts = [5568,3297,2338] #not 16\n",
    "# class_sample_counts = [450,237,113] #only one\n",
    "# weights = 1./ torch.tensor(class_sample_counts, dtype=torch.float)\n",
    "# # 这个 get_classes_for_all_imgs是关键\n",
    "# train_targets = train_data.get_classes_for_all_imgs()\n",
    "# samples_weights = weights[train_targets]\n",
    "\n",
    "# sampler = WeightedRandomSampler(weights=samples_weights, num_samples=len(samples_weights), replacement=True)\n",
    "\n",
    "\n",
    "\n",
    "#train_loader = DataLoader(train_data, batch_size = 4,sampler = sampler,shuffle=False)\n",
    "train_loader = DataLoader(train_data, batch_size = 4,shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多模态通道\n",
    "class DuECAAttention(nn.Module):\n",
    "    def __init__(self, c, b=1, gamma=2):\n",
    "        super(DuECAAttention,self).__init__()\n",
    "      \n",
    "        t = int(abs((math.log(c, 2) + b) / gamma))\n",
    "        k = t if t % 2 else t + 1 # k只能取奇数\n",
    "        # print(k,\"k\")\n",
    "        self.avg_pool_ch1 = nn.AdaptiveAvgPool2d(1)\n",
    "        self.avg_pool_ch2 = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        self.pad = nn.ZeroPad2d(padding=(int(k/2), int(k/2),0, 0))\n",
    "        \n",
    "        # self.conv1_ch1 = nn.Conv1d(1, 1, kernel_size=k, padding=int(k/2), bias=False,stride=2) #inchannel = 1,outchannel=1\n",
    "        # self.conv1_ch2 = nn.Conv1d(1, 1, kernel_size=k, padding=int(k/2), bias=False,stride=2) #inchannel = 1,outchannel=1\n",
    "        self.conv2_ch1 = nn.Conv2d(1, 1, kernel_size=(2,k), bias=False,stride=1) #inchannel = 1,outchannel=1\n",
    "        self.conv2_ch2 = nn.Conv2d(1, 1, kernel_size=(2,k), bias=False,stride=1) #inchannel = 1,outchannel=1\n",
    "        self.sigmoid_ch1 = nn.Sigmoid()\n",
    "        self.sigmoid_ch2 = nn.Sigmoid()\n",
    "        # self.softmax_ch1 = nn.Softmax()\n",
    "        # self.softmax_ch2 = nn.Softmax()\n",
    "        \n",
    "        \n",
    "    #定义网络的前向传播\n",
    "    def forward(self,inp_ch1, inp_ch2):\n",
    "        squeeze_ch1 = self.avg_pool_ch1(inp_ch1) #(n,c,h,w)->(n,c,1,1)\n",
    "        # print(\"squeeze_ch1\",squeeze_ch1.shape)\n",
    "        squeeze_ch2 = self.avg_pool_ch2(inp_ch2)\n",
    "        # print(\"squeeze_ch2\",squeeze_ch2.shape)\n",
    "        squeeze_comb = torch.cat((squeeze_ch1, squeeze_ch2), 1)  # [B, C*2]\n",
    "        # print(\"squeeze_comb\",squeeze_comb.shape)\n",
    "        squeeze_comb_2 = torch.cat((squeeze_ch1, squeeze_ch2), 2) \n",
    "        # print(\"squeeze_comb_2\",squeeze_comb_2.shape)\n",
    "        # fc_ch1_old = self.conv1_ch1(squeeze_comb.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n",
    "        # fc_ch2_old = self.conv1_ch2(squeeze_comb.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n",
    "        fc_ch1 = self.conv2_ch1(self.pad(squeeze_comb_2.transpose(-1, -3))).transpose(-1, -3)\n",
    "        fc_ch2 = self.conv2_ch2(self.pad(squeeze_comb_2.transpose(-1, -3))).transpose(-1, -3)\n",
    "        # print(\"fc_ch1_old\",fc_ch1_old.shape)\n",
    "        # print(\"fc_ch1\",fc_ch1.shape)\n",
    "        # print(\"fc_comb\",fc_comb.shape)\n",
    "        out_ch1 = self.sigmoid_ch1(fc_ch1)\n",
    "        out_ch2 = self.sigmoid_ch1(fc_ch1)\n",
    "        # out_ch1 = self.softmax_ch1(fc_ch1)\n",
    "        # out_ch2 = self.softmax_ch2(fc_ch2)\n",
    "        return out_ch1,out_ch2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多模态空间\n",
    "class DuSpatialAttention(nn.Module):\n",
    "    def __init__(self, c, b=1, gamma=2):\n",
    "        super(DuSpatialAttention,self).__init__()\n",
    "      \n",
    "        self.conv1_ch1 = nn.Conv2d(in_channels=4, out_channels=1, kernel_size=7, padding=7 // 2, bias=False)\n",
    "        self.conv1_ch2 = nn.Conv2d(in_channels=4, out_channels=1, kernel_size=7, padding=7 // 2, bias=False)\n",
    "        self.sigmoid_ch1 = nn.Sigmoid()\n",
    "        self.sigmoid_ch2 = nn.Sigmoid()\n",
    "        \n",
    "    #定义网络的前向传播\n",
    "    def forward(self,inp_ch1, inp_ch2):\n",
    "        \n",
    "        # 压缩通道提取空间信息\n",
    "        max_out_ch1, _ = torch.max(inp_ch1, dim=1, keepdim=True)\n",
    "        avg_out_ch1 = torch.mean(inp_ch1, dim=1, keepdim=True) #(b,1,h,w)\n",
    "        x_ch1 = torch.cat([max_out_ch1, avg_out_ch1], dim=1)#(b,2,h,w)\n",
    "        max_out_ch2, _ = torch.max(inp_ch2, dim=1, keepdim=True)\n",
    "        avg_out_ch2 = torch.mean(inp_ch2, dim=1, keepdim=True)\n",
    "        x_ch2 = torch.cat([max_out_ch2, avg_out_ch2], dim=1)#(b,2,h,w)\n",
    "        # 压缩结果拼接\n",
    "        comb = torch.cat((x_ch1, x_ch2), 1)#(b,4,h,w)\n",
    "        # 经过卷积提取空间注意力权重\n",
    "        out_ch1 = self.conv1_ch1(comb)\n",
    "        out_ch2 = self.conv1_ch2(comb)\n",
    "        # 输出非负\n",
    "        out_ch1 = self.sigmoid_ch1(out_ch1)\n",
    "        out_ch2 = self.sigmoid_ch2(out_ch2)\n",
    "        return out_ch1,out_ch2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuBasicBlock(nn.Module):      # 左侧的 residual block 结构（18-layer、34-layer）\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):      # 两层卷积 Conv2d + Shutcuts\n",
    "        super(DuBasicBlock, self).__init__()\n",
    "        \n",
    "        self.conv1_ch1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.conv1_ch2 = nn.Conv2d(in_planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        \n",
    "        self.bn1_ch1 = nn.BatchNorm2d(planes)\n",
    "        self.bn1_ch2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.conv2_ch1 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.conv2_ch2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        \n",
    "        self.bn2_ch1 = nn.BatchNorm2d(planes)\n",
    "        self.bn2_ch2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.channel = DuECAAttention(planes)       # Efficient Channel Attention module\n",
    "        self.spatial = DuSpatialAttention(planes)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:      # Shutcuts用于构建 Conv Block 和 Identity Block\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "            \n",
    "\n",
    "    def forward(self, c):\n",
    "        c1,c2 = c.chunk(2,dim=1)\n",
    "        # print(\"c1shape:\",c1.shape)\n",
    "        # print(\"c2shape:\",c2.shape)\n",
    "        \n",
    "        out1 = F.relu(self.bn1_ch1(self.conv1_ch1(c1)))\n",
    "        out2 = F.relu(self.bn1_ch2(self.conv1_ch2(c2)))\n",
    "        \n",
    "        out1 = self.bn2_ch1(self.conv2_ch1(out1))\n",
    "        out2 = self.bn2_ch2(self.conv2_ch2(out2))\n",
    "        \n",
    "        ECA_out1,ECA_out2 = self.channel(out1,out2)\n",
    "        \n",
    "        out1 = out1 * ECA_out1\n",
    "        out2 = out2 * ECA_out2\n",
    "        \n",
    "        Spa_out1,Spa_out2 = self.spatial(out1,out2)\n",
    "        out1 = out1*Spa_out1\n",
    "        out2 = out2*Spa_out2\n",
    "        \n",
    "        out1 += self.shortcut(c1)\n",
    "        out1 += self.shortcut(c2)\n",
    "        \n",
    "        out1 = F.relu(out1)\n",
    "        out2 = F.relu(out2)\n",
    "        \n",
    "        out = torch.cat((out1,out2),dim =1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECA_ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=3): #block有两种，BasicBlock或者Bottleneck，每个block里都有一个通道注意力\n",
    "        super(ECA_ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1_ch1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)                  # conv1\n",
    "        self.conv1_ch2 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)  \n",
    "        \n",
    "        self.bn1_ch1 = nn.BatchNorm2d(64)\n",
    "        self.bn1_ch2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)       # conv2_x\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)      # conv3_x\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)      # conv4_x\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)      # conv5_x\n",
    "        \n",
    "        self.avgpool_ch1 = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.avgpool_ch2 = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # self.linear_ch1 = nn.Linear(512 * block.expansion, num_classes)\n",
    "        # self.linear_ch2 = nn.Linear(512 * block.expansion, num_classes)\n",
    "        \n",
    "        self.linear =  nn.Linear(1024 * block.expansion, 2)\n",
    "        \n",
    "        # self.fc = nn.Linear(6, 3, bias=True)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def MA(self, x, label):\n",
    "        # x (k, v), label (q)\n",
    "        B, C_kv = x.shape\n",
    "        B, C_q = label.shape\n",
    "        self.kv = nn.Linear(C_kv, C_kv * 3 * 2).cuda()\n",
    "        self.q = nn.Linear(C_q, C_kv * 3).cuda()\n",
    "        self.at_fx = nn.Linear(C_kv * 3, C_kv).cuda()\n",
    "        #self.ffn = nn.Linear(C_kv, C_kv).cuda()\n",
    "        kv = self.kv(x).reshape(2, B, 3, C_kv)\n",
    "        k, v = kv[0], kv[1]\n",
    "        q = self.q(label).reshape(B, 3, C_kv)\n",
    "        attn = torch.einsum(\"bhq,bhk->bhqk\", [q, k])\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x_ = torch.einsum(\"bhqk,bhk->bhq\", [attn, v])\n",
    "        x_ = x_.reshape(B, C_kv * 3)\n",
    "        x = self.at_fx(x_) + x\n",
    "        #x = self.ffn(x) + x\n",
    "        return x \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, c1,c2):\n",
    "        c1 = F.relu(self.bn1_ch1(self.conv1_ch1(c1)))\n",
    "        c2 = F.relu(self.bn1_ch2(self.conv1_ch2(c2))) \n",
    "        \n",
    "        # nn.Sequential()定义的网络，只能接受单输入，同时输出也只能是单输出。  \n",
    "        # 解决方法，可以将两个tensor经过torch.cat组合在一起，然后在forward()中将组合拆分\n",
    "        \n",
    "        c = torch.cat((c1,c2),dim =1)\n",
    "        \n",
    "        c = self.layer1(c)\n",
    "        c = self.layer2(c)\n",
    "        c = self.layer3(c)\n",
    "        c = self.layer4(c)\n",
    "        \n",
    "        c1,c2 = c.chunk(2,dim=1)\n",
    "        \n",
    "        c1 = self.avgpool_ch1(c1)\n",
    "        c2 = self.avgpool_ch2(c2)\n",
    "        \n",
    "        c1 = torch.flatten(c1, 1)\n",
    "        c2 = torch.flatten(c2, 1)\n",
    "        \n",
    "        # c1 = self.linear_ch1(c1)\n",
    "        # c2 = self.linear_ch2(c2)\n",
    "        \n",
    "        \n",
    "               \n",
    "        c1_ma = self.MA(c1,c2)\n",
    "        c2_ma = self.MA(c2,c1)\n",
    "        \n",
    "        \n",
    "        comb = torch.cat((c1_ma, c2_ma), 1)\n",
    "        out = self.linear(comb)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ECA_ResNet18():\n",
    "    return ECA_ResNet(DuBasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECAfusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ECAfusionModel,self).__init__()\n",
    "      \n",
    "        self.ECA = ECA_ResNet18()\n",
    "        \n",
    "    #定义网络的前向传播\n",
    "    def forward(self,MRI,PET):\n",
    "        output = self.ECA(MRI,PET)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train_loss: 0.6074750265520164 train_acc: tensor(0.6753, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 0 val_loss: 0.502542086483277 val_acc: tensor(0.8557, device='cuda:0', dtype=torch.float64)\n",
      "save model\n",
      "epoch: 1 train_loss: 0.4883959745192418 train_acc: tensor(0.7795, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 1 val_loss: 0.4436108204200096 val_acc: tensor(0.7577, device='cuda:0', dtype=torch.float64)\n",
      "save model\n",
      "epoch: 2 train_loss: 0.4233296372076271 train_acc: tensor(0.8147, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 2 val_loss: 1.9156422510855056 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 3 train_loss: 0.34174717157212003 train_acc: tensor(0.8591, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 3 val_loss: 1.9759659732808266 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 4 train_loss: 0.30571135753055445 train_acc: tensor(0.8943, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 4 val_loss: 0.3545634150505066 val_acc: tensor(0.8763, device='cuda:0', dtype=torch.float64)\n",
      "save model\n",
      "epoch: 5 train_loss: 0.23997828497866577 train_acc: tensor(0.9066, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 5 val_loss: 0.3361775696892099 val_acc: tensor(0.8351, device='cuda:0', dtype=torch.float64)\n",
      "save model\n",
      "epoch: 6 train_loss: 0.23003249190987343 train_acc: tensor(0.9127, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 6 val_loss: 3.308120284497914 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 7 train_loss: 0.2330213191920455 train_acc: tensor(0.9142, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 7 val_loss: 0.5720363956175207 val_acc: tensor(0.6443, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 8 train_loss: 0.19581638122545084 train_acc: tensor(0.9219, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 8 val_loss: 0.7888287470421571 val_acc: tensor(0.6237, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 9 train_loss: 0.20114112281415947 train_acc: tensor(0.9280, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 9 val_loss: 1.7750645038471815 val_acc: tensor(0.5464, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 10 train_loss: 0.17240017454837311 train_acc: tensor(0.9418, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 10 val_loss: 2.5669604155182135 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 11 train_loss: 0.12783146150421584 train_acc: tensor(0.9632, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 11 val_loss: 1.9551410849080546 val_acc: tensor(0.5464, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 12 train_loss: 0.1888504998350536 train_acc: tensor(0.9234, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 12 val_loss: 0.8199972735235429 val_acc: tensor(0.6237, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 13 train_loss: 0.15112580098554057 train_acc: tensor(0.9372, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 13 val_loss: 0.70196544455746 val_acc: tensor(0.6804, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 14 train_loss: 0.19829218786717556 train_acc: tensor(0.9096, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 14 val_loss: 0.655139708451053 val_acc: tensor(0.6392, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 15 train_loss: 0.1482566105198988 train_acc: tensor(0.9372, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 15 val_loss: 0.9730074415222674 val_acc: tensor(0.6495, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 16 train_loss: 0.16873939884726705 train_acc: tensor(0.9357, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 16 val_loss: 0.5986051884451017 val_acc: tensor(0.7216, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 17 train_loss: 0.14696066768392121 train_acc: tensor(0.9403, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 17 val_loss: 0.31151411368406 val_acc: tensor(0.8402, device='cuda:0', dtype=torch.float64)\n",
      "save model\n",
      "epoch: 18 train_loss: 0.11862607181477282 train_acc: tensor(0.9495, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 18 val_loss: 0.8650641997924469 val_acc: tensor(0.6237, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 19 train_loss: 0.15103209617149774 train_acc: tensor(0.9296, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 19 val_loss: 3.9716043029863353 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 20 train_loss: 0.15426706520278594 train_acc: tensor(0.9326, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 20 val_loss: 3.4855963507877754 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 21 train_loss: 0.10661590960843755 train_acc: tensor(0.9617, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 21 val_loss: 2.0488351963283002 val_acc: tensor(0.5464, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 22 train_loss: 0.14659135193231687 train_acc: tensor(0.9342, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 22 val_loss: 1.4439462102022047 val_acc: tensor(0.5825, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 23 train_loss: 0.12074721803543532 train_acc: tensor(0.9556, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 23 val_loss: 0.7757155946436394 val_acc: tensor(0.6392, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 24 train_loss: 0.15808520465376139 train_acc: tensor(0.9265, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 24 val_loss: 0.33606947437120777 val_acc: tensor(0.8557, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 25 train_loss: 0.1278124870832262 train_acc: tensor(0.9433, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 25 val_loss: 3.430406848794405 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 26 train_loss: 0.17203944062765625 train_acc: tensor(0.9127, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 26 val_loss: 4.927388212115495 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 27 train_loss: 0.16682804322974104 train_acc: tensor(0.9158, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 27 val_loss: 3.4293348561855335 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 28 train_loss: 0.17667279278097894 train_acc: tensor(0.9127, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 28 val_loss: 1.6697993515584684 val_acc: tensor(0.5876, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 29 train_loss: 0.1347522080824589 train_acc: tensor(0.9311, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 29 val_loss: 2.3464932062633563 val_acc: tensor(0.5464, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 30 train_loss: 0.12988284535087063 train_acc: tensor(0.9433, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 30 val_loss: 0.4096666268197839 val_acc: tensor(0.8247, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 31 train_loss: 0.14336133615155155 train_acc: tensor(0.9311, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 31 val_loss: 1.7988328126996294 val_acc: tensor(0.5876, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 32 train_loss: 0.122415686402738 train_acc: tensor(0.9433, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 32 val_loss: 3.215299882839539 val_acc: tensor(0.5464, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 33 train_loss: 0.11245906482925595 train_acc: tensor(0.9495, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 33 val_loss: 2.7409893329557162 val_acc: tensor(0.5464, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 34 train_loss: 0.12036610754052744 train_acc: tensor(0.9449, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 34 val_loss: 3.6881692501686496 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 35 train_loss: 0.09038469809532497 train_acc: tensor(0.9632, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 35 val_loss: 3.1472417361955225 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 36 train_loss: 0.12593618825942302 train_acc: tensor(0.9418, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 36 val_loss: 60.88362947444326 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 37 train_loss: 0.13734399910679637 train_acc: tensor(0.9372, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 37 val_loss: 1.5320788141355564 val_acc: tensor(0.5979, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 38 train_loss: 0.12016942988793047 train_acc: tensor(0.9556, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 38 val_loss: 2.7051660338606207 val_acc: tensor(0.5464, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 39 train_loss: 0.07610666371880535 train_acc: tensor(0.9678, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 39 val_loss: 3.1545434784642192 val_acc: tensor(0.5464, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 40 train_loss: 0.12214441870547443 train_acc: tensor(0.9418, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 40 val_loss: 0.8467748302304169 val_acc: tensor(0.6392, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 41 train_loss: 0.09871440073389669 train_acc: tensor(0.9587, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 41 val_loss: 2.962747971420202 val_acc: tensor(0.5464, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 42 train_loss: 0.07988621635008802 train_acc: tensor(0.9724, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 42 val_loss: 3.2683491977209203 val_acc: tensor(0.5464, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 43 train_loss: 0.0888437273025022 train_acc: tensor(0.9678, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 43 val_loss: 2.3377670863822146 val_acc: tensor(0.5876, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 44 train_loss: 0.10560731182001805 train_acc: tensor(0.9587, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 44 val_loss: 4.077168057874293 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 45 train_loss: 0.0936108369117811 train_acc: tensor(0.9571, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 45 val_loss: 6.334561920657601 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 46 train_loss: 0.059381309839658676 train_acc: tensor(0.9816, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 46 val_loss: 1.9991471491672885 val_acc: tensor(0.5876, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 47 train_loss: 0.10328769531588756 train_acc: tensor(0.9663, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 47 val_loss: 5.300384548521533 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 48 train_loss: 0.1010524611898725 train_acc: tensor(0.9571, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 48 val_loss: 4.9372044303982525 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 49 train_loss: 0.14285849455521538 train_acc: tensor(0.9464, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 49 val_loss: 4.948586611403632 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 50 train_loss: 0.1189195253960446 train_acc: tensor(0.9510, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 50 val_loss: 2.727996350505629 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 51 train_loss: 0.05967253825240761 train_acc: tensor(0.9816, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 51 val_loss: 2.9094622638945324 val_acc: tensor(0.5464, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 52 train_loss: 0.11546062275619 train_acc: tensor(0.9495, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 52 val_loss: 1.6999660113679524 val_acc: tensor(0.5464, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 53 train_loss: 0.0809088748185161 train_acc: tensor(0.9724, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 53 val_loss: 3.451421070344472 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 54 train_loss: 0.07674309328607759 train_acc: tensor(0.9709, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 54 val_loss: 2.4911118860716495 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 55 train_loss: 0.07999590143224775 train_acc: tensor(0.9632, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 55 val_loss: 2.4124149495465375 val_acc: tensor(0.5464, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 56 train_loss: 0.08098396181576753 train_acc: tensor(0.9770, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 56 val_loss: 3.733805929262586 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 57 train_loss: 0.08975810837321509 train_acc: tensor(0.9663, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 57 val_loss: 3.8104456171547 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 58 train_loss: 0.0593146107849133 train_acc: tensor(0.9801, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 58 val_loss: 1.9350408918795325 val_acc: tensor(0.5876, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 59 train_loss: 0.09964888924083874 train_acc: tensor(0.9510, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 59 val_loss: 1.7892422602038742 val_acc: tensor(0.5567, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 60 train_loss: 0.06248742708106273 train_acc: tensor(0.9740, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 60 val_loss: 1.893390127342047 val_acc: tensor(0.5515, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 61 train_loss: 0.10853243991304731 train_acc: tensor(0.9510, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 61 val_loss: 1.3982739618279327 val_acc: tensor(0.5825, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 62 train_loss: 0.07057728722604537 train_acc: tensor(0.9786, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 62 val_loss: 2.8216694998971623 val_acc: tensor(0.5464, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 63 train_loss: 0.05870050392584279 train_acc: tensor(0.9770, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 63 val_loss: 2.7062957385095645 val_acc: tensor(0.5464, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 64 train_loss: 0.06981482445355076 train_acc: tensor(0.9770, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 64 val_loss: 2.8748723139096874 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 65 train_loss: 0.05621190197875646 train_acc: tensor(0.9893, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 65 val_loss: 2.269740787248924 val_acc: tensor(0.5464, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 66 train_loss: 0.039345101826677345 train_acc: tensor(0.9908, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 66 val_loss: 4.1906828253539565 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 67 train_loss: 0.030716942026116564 train_acc: tensor(0.9954, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 67 val_loss: 5.6697699071205765 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 68 train_loss: 0.06978055182327395 train_acc: tensor(0.9755, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 68 val_loss: 2.3888186633803623 val_acc: tensor(0.5515, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 69 train_loss: 0.04640701316421136 train_acc: tensor(0.9893, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 69 val_loss: 3.6132718502860306 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 70 train_loss: 0.033159711722827945 train_acc: tensor(0.9923, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 70 val_loss: 7.508591369255302 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 71 train_loss: 0.047118461946493294 train_acc: tensor(0.9923, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 71 val_loss: 3.6558281084925754 val_acc: tensor(0.5464, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 72 train_loss: 0.06544325454784543 train_acc: tensor(0.9755, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 72 val_loss: 3.4420326812930675 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 73 train_loss: 0.03768796168423185 train_acc: tensor(0.9893, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 73 val_loss: 3.1093383818537372 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 74 train_loss: 0.036374596754804964 train_acc: tensor(0.9908, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 74 val_loss: 3.523992323998317 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 75 train_loss: 0.024342503314688062 train_acc: tensor(1., device='cuda:0', dtype=torch.float64)\n",
      "epoch: 75 val_loss: 4.141935484925495 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 76 train_loss: 0.018650055234356117 train_acc: tensor(0.9985, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 76 val_loss: 10.701267149030548 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 77 train_loss: 0.04258118840351638 train_acc: tensor(0.9862, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 77 val_loss: 2.095498362789118 val_acc: tensor(0.5464, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 78 train_loss: 0.022972538852087217 train_acc: tensor(0.9985, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 78 val_loss: 4.246535782347017 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 79 train_loss: 0.022014821717867863 train_acc: tensor(0.9969, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 79 val_loss: 3.548253308866327 val_acc: tensor(0.5464, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 80 train_loss: 0.03903605354376491 train_acc: tensor(0.9908, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 80 val_loss: 4.659372870454098 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 81 train_loss: 0.016674399858513194 train_acc: tensor(1., device='cuda:0', dtype=torch.float64)\n",
      "epoch: 81 val_loss: 3.344397869920708 val_acc: tensor(0.5464, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 82 train_loss: 0.05894992771008655 train_acc: tensor(0.9740, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 82 val_loss: 1.7897674313591272 val_acc: tensor(0.5515, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 83 train_loss: 0.027562088890990263 train_acc: tensor(0.9954, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 83 val_loss: 2.587537520194108 val_acc: tensor(0.5464, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 84 train_loss: 0.03786217111963665 train_acc: tensor(0.9862, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 84 val_loss: 3.724282619878759 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 85 train_loss: 0.020831401069355476 train_acc: tensor(0.9939, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 85 val_loss: 5.048902777052417 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 86 train_loss: 0.024394112645903715 train_acc: tensor(0.9923, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 86 val_loss: 5.646280033072245 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 87 train_loss: 0.02117918611872816 train_acc: tensor(0.9969, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 87 val_loss: 4.362561740826085 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 88 train_loss: 0.016324529521114507 train_acc: tensor(0.9969, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 88 val_loss: 4.794478839820194 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 89 train_loss: 0.019430349752001013 train_acc: tensor(0.9985, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 89 val_loss: 3.8423462786625224 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 90 train_loss: 0.014766749590749398 train_acc: tensor(0.9969, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 90 val_loss: 5.10350408013334 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 91 train_loss: 0.010865627546931274 train_acc: tensor(0.9985, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 91 val_loss: 6.341140793770859 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 92 train_loss: 0.007853810926546323 train_acc: tensor(1., device='cuda:0', dtype=torch.float64)\n",
      "epoch: 92 val_loss: 4.0374354345282235 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 93 train_loss: 0.0045597738747083505 train_acc: tensor(1., device='cuda:0', dtype=torch.float64)\n",
      "epoch: 93 val_loss: 4.886405853881049 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 94 train_loss: 0.009234571456303036 train_acc: tensor(0.9969, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 94 val_loss: 2.629350284568823 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 95 train_loss: 0.031345504772750256 train_acc: tensor(0.9939, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 95 val_loss: 4.471863476271482 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 96 train_loss: 0.009953550092311175 train_acc: tensor(0.9985, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 96 val_loss: 6.51031449160625 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 97 train_loss: 0.007282296014407002 train_acc: tensor(1., device='cuda:0', dtype=torch.float64)\n",
      "epoch: 97 val_loss: 5.80190508144418 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 98 train_loss: 0.0059525196742342005 train_acc: tensor(1., device='cuda:0', dtype=torch.float64)\n",
      "epoch: 98 val_loss: 4.8549527310833485 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n",
      "epoch: 99 train_loss: 0.0049197687077065745 train_acc: tensor(1., device='cuda:0', dtype=torch.float64)\n",
      "epoch: 99 val_loss: 5.139393967451508 val_acc: tensor(0.5412, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "model = ECAfusionModel()\n",
    "model = model.to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "\n",
    "#记录训练过程指标\n",
    "historyl = hl.History()\n",
    "#使用Canves进行可视化\n",
    "canvasl = hl.Canvas()\n",
    "\n",
    "min_loss = 2\n",
    "\n",
    "#对模型进行迭代训练，对所有的数据训练epoch轮\n",
    "for epoch in range(100):\n",
    "    train_loss_epoch = 0\n",
    "    val_loss_epoch = 0\n",
    "    train_corrects = 0\n",
    "    val_corrects = 0\n",
    "    #对训练数据的加载器进行迭代计算\n",
    "    model.train().cuda()\n",
    "    for step,(mri,pet,group) in enumerate(train_loader):\n",
    "        ##计算每个batch的损失\n",
    "        mri = mri.to(DEVICE)\n",
    "        pet = pet.to(DEVICE)\n",
    "        group = group.to(DEVICE)\n",
    "        output = model(mri,pet)\n",
    "        loss = criterion(output,group)#交叉熵损失函数\n",
    "        pre_lab = torch.argmax(output,1)\n",
    "        optimizer.zero_grad()#每个迭代步的梯度初始化为0\n",
    "        loss.backward()#损失的后向传播，计算梯度\n",
    "        optimizer.step()#使用梯度进行优化\n",
    "        train_loss_epoch += loss.item()*group.size(0)\n",
    "        train_corrects += torch.sum(pre_lab == group.to(DEVICE).data)\n",
    "    #计算一个epoch的损失和精度\n",
    "    train_loss = train_loss_epoch/len(train_data.group)\n",
    "    train_acc = train_corrects.double()/len(train_data.group)\n",
    "    print(\"epoch:\",epoch,\"train_loss:\",train_loss,\"train_acc:\",train_acc)\n",
    "    \n",
    "    #计算在验证集上的表现\n",
    "    model.eval()\n",
    "    for step,(mri,pet,group) in enumerate(test_loader):\n",
    "        mri = mri.to(DEVICE)\n",
    "        pet = pet.to(DEVICE)\n",
    "        group = group.to(DEVICE)\n",
    "        output = model(mri,pet)\n",
    "        loss = criterion(output,group.to(DEVICE))\n",
    "        pre_lab = torch.argmax(output,1).to(DEVICE)\n",
    "        val_loss_epoch += loss.item()*group.size(0)\n",
    "        val_corrects += torch.sum(pre_lab == group.to(DEVICE).data)\n",
    "        # print(\"valid real\",group)\n",
    "        # print(\"valid pred\",pre_lab)\n",
    "    #计算一个epoch上的输出loss和acc\n",
    "    val_loss = val_loss_epoch/len(test_data.group)\n",
    "    val_acc = val_corrects.double()/len(test_data.group)\n",
    "    print(\"epoch:\",epoch,\"val_loss:\",val_loss,\"val_acc:\",val_acc)\n",
    "    \n",
    "    \n",
    "    if val_loss < min_loss:\n",
    "        min_loss = val_loss\n",
    "        print(\"save model\")\n",
    "        # 保存模型语句\n",
    "        torch.save(model.state_dict(),\"model_\"+str(val_acc)+\".pth\")\n",
    "    # #保存每个epoch上的输出loss和acc\n",
    "    historyl.log(epoch,train_loss=train_loss,val_loss = val_loss,train_acc = train_acc.item(),val_acc = val_acc.item())\n",
    "    # #可视化网络训练的过程\n",
    "    # with canvasl:\n",
    "    #     canvasl.draw_plot([historyl[\"train_loss\"],historyl[\"val_loss\"]])\n",
    "    #     canvasl.draw_plot([historyl[\"train_acc\"],historyl[\"val_acc\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: \"/home/gc/gechang/final_final/filelist/two_class/ADCN/model_tensor(0.9330, device='cuda:0', dtype=torch.float64).pth\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36635/446599323.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mECAfusionModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/gc/gechang/final_final/filelist/two_class/ADCN/model_tensor(0.9330, device=\\'cuda:0\\', dtype=torch.float64).pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py372/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py372/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py372/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: \"/home/gc/gechang/final_final/filelist/two_class/ADCN/model_tensor(0.9330, device='cuda:0', dtype=torch.float64).pth\""
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#加载模型\n",
    "\n",
    "\n",
    "model = ECAfusionModel().cuda()\n",
    " \n",
    "model.load_state_dict(torch.load('/home/gc/gechang/final_final/filelist/two_class/ADCN/model_tensor(0.9330, device=\\'cuda:0\\', dtype=torch.float64).pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MRI_PET= pd.read_excel(\"/home/gc/gechang/final_final/filelist/two_class/ADCN/testadcn.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in MRI_PET.iterrows():\n",
    "    if (row[\"Group\"] == 1):\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import GradCAM, show_cam_on_image\n",
    "import numpy as np\n",
    "target_layers = [model.ECA.layer4]\n",
    "\n",
    "\n",
    "# load image\n",
    "\n",
    "\n",
    "MRI_img = torch.from_numpy(MRI_PET.iloc[31]['MRI_img_array'].transpose(2,0,1)).float()\n",
    "PET_img = torch.from_numpy(MRI_PET.iloc[31]['PET_img_array'].transpose(2,0,1)).float()\n",
    "\n",
    "print(MRI_PET.iloc[31]['Group'])\n",
    "\n",
    "# [N, C, H, W]\n",
    "\n",
    "\n",
    "\n",
    "# expand batch dimension\n",
    "input_tensor_MRI = torch.unsqueeze(MRI_img, dim=0).cuda()\n",
    "input_tensor_PET = torch.unsqueeze(PET_img, dim=0).cuda()\n",
    "\n",
    "\n",
    "cam = GradCAM(model=model, target_layers=target_layers, use_cuda=False)\n",
    "target_category = 2\n",
    "grayscale_cam = cam(MRI=input_tensor_MRI, PET =input_tensor_PET)\n",
    "\n",
    "grayscale_cam = grayscale_cam[0, :]\n",
    "visualization = show_cam_on_image(MRI_PET.iloc[53]['MRI_img_array'].astype(dtype=np.float32) / 255.,\n",
    "                                      grayscale_cam,\n",
    "                                      use_rgb=True)\n",
    "\n",
    "plt.imshow(visualization)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization1 = show_cam_on_image(MRI_PET.iloc[985]['PET_img_array'].astype(dtype=np.float32) / 255.,\n",
    "                                      grayscale_cam,\n",
    "                                      use_rgb=True)\n",
    "\n",
    "plt.imshow(visualization1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization2 = show_cam_on_image(MRI_PET.iloc[1]['PET_img_array'].astype(dtype=np.float32) / 255.,\n",
    "                                      grayscale_cam,\n",
    "                                      use_rgb=True)\n",
    "\n",
    "plt.imshow(visualization2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 混淆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制混淆矩阵\n",
    "def confusion_matrix(preds, labels, conf_matrix):\n",
    "    for p, t in zip(preds, labels):\n",
    "        conf_matrix[p, t] += 1\n",
    "    return conf_matrix\n",
    "conf_matrix = torch.zeros(2, 2)\n",
    "for mri,pet,group in test_loader:\n",
    "    output = model(mri.to(DEVICE),pet.to(DEVICE))\n",
    "    pre_lab = torch.argmax(output,1).to(DEVICE)\n",
    "    conf_matrix = confusion_matrix(pre_lab, group, conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制混淆矩阵\n",
    "import matplotlib.pyplot as plt\n",
    "Emotion_kinds=2#这个数值是具体的分类数，大家可以自行修改\n",
    "labels = ['AD', 'NC']#每种类别的标签\n",
    " \n",
    "# 显示数据\n",
    "plt.imshow(conf_matrix, cmap=plt.cm.Blues)\n",
    " \n",
    "# 在图中标注数量/概率信息\n",
    "thresh = conf_matrix.max() / 2  #数值颜色阈值，如果数值超过这个，就颜色加深。\n",
    "for x in range(Emotion_kinds):\n",
    "    for y in range(Emotion_kinds):\n",
    "        # 注意这里的matrix[y, x]不是matrix[x, y]\n",
    "        info = int(conf_matrix[y, x])\n",
    "        plt.text(x, y, info,\n",
    "                 verticalalignment='center',\n",
    "                 horizontalalignment='center',\n",
    "                 color=\"white\" if info > thresh else \"black\")\n",
    "                  \n",
    "plt.tight_layout()#保证图不重叠\n",
    "plt.yticks(range(Emotion_kinds), labels)\n",
    "plt.xticks(range(Emotion_kinds), labels,rotation=45)#X轴字体倾斜45°\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算模型参数量\n",
    "def count_param(model):\n",
    "    param_count = 0\n",
    "    for param in model.parameters():\n",
    "        param_count += param.view(-1).size()[0]\n",
    "    return param_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_param(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('py372')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9e0e02a671462d396ecc20af6161881d6338b1e899f7d69a261eb7f1f955476"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
